{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "image_caption_generator.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMVQgJZIvEXy",
        "colab_type": "text"
      },
      "source": [
        "# Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yaw-9EXubPf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive            # import drive from google colab\n",
        "ROOT = \"/content/drive\"                   # default location for the drive\n",
        "drive.mount(ROOT)                         # mount google drive at /content/drive\n",
        "%cd drive/'My Drive'/'Colab Notebooks'/'Image Caption Generator'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lowBOUc_zwAj",
        "colab_type": "text"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qrqhrtobzuw0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import zipfile\n",
        "import random\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from random import choice\n",
        "from collections import defaultdict\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "from tensorflow.contrib import keras\n",
        "import utils\n",
        "from keras_utils import reset_tf_session\n",
        "import tqdm_utils\n",
        "import download_utils\n",
        "L = keras.layers\n",
        "K = keras.backend"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SPh5zCyhwcH",
        "colab_type": "text"
      },
      "source": [
        "# Download Training and Validation Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8wuHl8-hw1a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def download_resources(save_path):\n",
        "    # Originals:\n",
        "    # http://msvocds.blob.core.windows.net/annotations-1-0-3/captions_train-val2014.zip\n",
        "    download_utils.sequential_downloader(\n",
        "        \"v0.1\",\n",
        "        [\n",
        "            \"captions_train-val2014.zip\",\n",
        "            \"train2014_sample.zip\",\n",
        "            \"train_img_embeds.pickle\",\n",
        "            \"train_img_fns.pickle\",\n",
        "            \"val2014_sample.zip\",\n",
        "            \"val_img_embeds.pickle\",\n",
        "            \"val_img_fns.pickle\"\n",
        "        ],\n",
        "        save_path\n",
        "    )\n",
        "\n",
        "download_resources(\"data\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pjy5ve3gAjJJ",
        "colab_type": "text"
      },
      "source": [
        "# Load Image Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bx0m4BeA9mY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load prepared embeddings\n",
        "train_img_embeds = utils.read_pickle(\"data/train_img_embeds.pickle\")\n",
        "train_img_fns = utils.read_pickle(\"data/train_img_fns.pickle\")\n",
        "val_img_embeds = utils.read_pickle(\"data/val_img_embeds.pickle\")\n",
        "val_img_fns = utils.read_pickle(\"data/val_img_fns.pickle\")\n",
        "# check shapes\n",
        "print(train_img_embeds.shape, len(train_img_fns))\n",
        "print(val_img_embeds.shape, len(val_img_fns))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gze1dJ1GHdPm",
        "colab_type": "text"
      },
      "source": [
        "# Load Captions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WSMHmQWHBn6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# extract captions from zip\n",
        "def get_captions_for_fns(fns, zip_fn, zip_json_path):\n",
        "    zf = zipfile.ZipFile(zip_fn)\n",
        "    j = json.loads(zf.read(zip_json_path).decode(\"utf8\"))\n",
        "    id_to_fn = {img[\"id\"]: img[\"file_name\"] for img in j[\"images\"]}\n",
        "    fn_to_caps = defaultdict(list)\n",
        "    for cap in j['annotations']:\n",
        "        fn_to_caps[id_to_fn[cap['image_id']]].append(cap['caption'])\n",
        "    fn_to_caps = dict(fn_to_caps)\n",
        "    return list(map(lambda x: fn_to_caps[x], fns))\n",
        "\n",
        "# load captions\n",
        "train_captions = get_captions_for_fns(train_img_fns, \"data/captions_train-val2014.zip\", \"annotations/captions_train2014.json\")\n",
        "val_captions = get_captions_for_fns(val_img_fns, \"data/captions_train-val2014.zip\", \"annotations/captions_val2014.json\")\n",
        "\n",
        "# check shape\n",
        "print(len(train_img_fns), len(train_captions))\n",
        "print(len(val_img_fns), len(val_captions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdskpnL-IPQW",
        "colab_type": "text"
      },
      "source": [
        "# Show Training Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrwlpGW4IUlh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# look at training example (each has 5 captions)\n",
        "def show_trainig_example(train_img_fns, train_captions, example_idx=0):\n",
        "    \"\"\"\n",
        "    You can change example_idx and see different images\n",
        "    \"\"\"\n",
        "    zf = zipfile.ZipFile(\"data/train2014_sample.zip\")\n",
        "    captions_by_file = dict(zip(train_img_fns, train_captions))\n",
        "    all_files = set(train_img_fns)\n",
        "    found_files = list(filter(lambda x: x.filename.rsplit(\"/\")[-1] in all_files, zf.filelist))\n",
        "    example = found_files[example_idx]\n",
        "    img = utils.decode_image_from_buf(zf.read(example))\n",
        "    plt.imshow(utils.image_center_crop(img))\n",
        "    plt.title(\"\\n\".join(captions_by_file[example.filename.rsplit(\"/\")[-1]]))\n",
        "    plt.show()\n",
        "    \n",
        "show_trainig_example(train_img_fns, train_captions, example_idx=149)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OenzQ64JIl4-",
        "colab_type": "text"
      },
      "source": [
        "# Prepare Data for Training and Making Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmJQ5xw7IroW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# special tokens\n",
        "PAD = \"#PAD#\"\n",
        "UNK = \"#UNK#\"\n",
        "START = \"#START#\"\n",
        "END = \"#END#\"\n",
        "\n",
        "def split_sentence(sentence):\n",
        "    return list(filter(lambda x: len(x) > 0, re.split('\\W+', sentence.lower())))\n",
        "\n",
        "def generate_vocabulary(train_captions):\n",
        "    \"\"\"\n",
        "    Return {token: index} for all train tokens (words) that occur 5 times or more, \n",
        "        `index` should be from 0 to N, where N is a number of unique tokens in the resulting dictionary.\n",
        "    Use `split_sentence` function to split sentence into tokens.\n",
        "    Also, add PAD (for batch padding), UNK (unknown, out of vocabulary), \n",
        "        START (start of sentence) and END (end of sentence) tokens into the vocabulary.\n",
        "    \"\"\"\n",
        "    vocab = [PAD, UNK, START, END]\n",
        "    freq = dict()\n",
        "    for image in train_captions:\n",
        "        for sentence in image:\n",
        "            for token in split_sentence(sentence):\n",
        "                freq[token] = freq[token]+1 if token in freq else 1\n",
        "                if freq[token] == 5: vocab.append(token)\n",
        "    return {token: index for index, token in enumerate(sorted(vocab))}\n",
        "    \n",
        "def caption_tokens_to_indices(captions, vocab):\n",
        "    \"\"\"\n",
        "    `captions` argument is an array of arrays:\n",
        "    [\n",
        "        [\n",
        "            \"image1 caption1\",\n",
        "            \"image1 caption2\",\n",
        "            ...\n",
        "        ],\n",
        "        [\n",
        "            \"image2 caption1\",\n",
        "            \"image2 caption2\",\n",
        "            ...\n",
        "        ],\n",
        "        ...\n",
        "    ]\n",
        "    Use `split_sentence` function to split sentence into tokens.\n",
        "    Replace all tokens with vocabulary indices, use UNK for unknown words (out of vocabulary).\n",
        "    Add START and END tokens to start and end of each sentence respectively.\n",
        "    For the example above you should produce the following:\n",
        "    [\n",
        "        [\n",
        "            [vocab[START], vocab[\"image1\"], vocab[\"caption1\"], vocab[END]],\n",
        "            [vocab[START], vocab[\"image1\"], vocab[\"caption2\"], vocab[END]],\n",
        "            ...\n",
        "        ],\n",
        "        ...\n",
        "    ]\n",
        "    \"\"\"\n",
        "    res = []\n",
        "    for image in captions:\n",
        "        res.append([])\n",
        "        for sentence in image:\n",
        "            res[-1].append([vocab[START]])\n",
        "            for token in split_sentence(sentence):\n",
        "                res[-1][-1].append(vocab[token] if token in vocab else vocab[UNK])\n",
        "            res[-1][-1].append(vocab[END])\n",
        "    return res\n",
        "\n",
        "def batch_captions_to_matrix(batch_captions, pad_idx, max_len=None):\n",
        "    \"\"\"\n",
        "    `batch_captions` is an array of arrays:\n",
        "    [\n",
        "        [vocab[START], ..., vocab[END]],\n",
        "        [vocab[START], ..., vocab[END]],\n",
        "        ...\n",
        "    ]\n",
        "    Put vocabulary indexed captions into np.array of shape (len(batch_captions), columns),\n",
        "        where \"columns\" is max(map(len, batch_captions)) when max_len is None\n",
        "        and \"columns\" = min(max_len, max(map(len, batch_captions))) otherwise.\n",
        "    Add padding with pad_idx where necessary.\n",
        "    Input example: [[1, 2, 3], [4, 5]]\n",
        "    Output example: np.array([[1, 2, 3], [4, 5, pad_idx]]) if max_len=None\n",
        "    Output example: np.array([[1, 2], [4, 5]]) if max_len=2\n",
        "    Output example: np.array([[1, 2, 3], [4, 5, pad_idx]]) if max_len=100\n",
        "    Try to use numpy, we need this function to be fast!\n",
        "    \"\"\"\n",
        "    columns = max(map(len, batch_captions)) if max_len == None else min(max_len, max(map(len, batch_captions)))\n",
        "    matrix = np.zeros((len(batch_captions), columns)) + pad_idx\n",
        "    for i in range(len(batch_captions)):\n",
        "        for j in range(min(len(batch_captions[i]), columns)):\n",
        "            matrix[i][j] = batch_captions[i][j]\n",
        "    return matrix\n",
        "\n",
        "def generate_batch(images_embeddings, indexed_captions, batch_size, max_len=None):\n",
        "    \"\"\"\n",
        "    `images_embeddings` is a np.array of shape [number of images, IMG_EMBED_SIZE].\n",
        "    `indexed_captions` holds 5 vocabulary indexed captions for each image:\n",
        "    [\n",
        "        [\n",
        "            [vocab[START], vocab[\"image1\"], vocab[\"caption1\"], vocab[END]],\n",
        "            [vocab[START], vocab[\"image1\"], vocab[\"caption2\"], vocab[END]],\n",
        "            ...\n",
        "        ],\n",
        "        ...\n",
        "    ]\n",
        "    Generate a random batch of size `batch_size`.\n",
        "    Take random images and choose one random caption for each image.\n",
        "    Remember to use `batch_captions_to_matrix` for padding and respect `max_len` parameter.\n",
        "    Return feed dict {decoder.img_embeds: ..., decoder.sentences: ...}.\n",
        "    \"\"\"\n",
        "    indices = np.random.choice(len(indexed_captions), batch_size, replace=False)\n",
        "    \n",
        "    batch_image_embeddings = images_embeddings[indices]\n",
        "    batch_captions_matrix = [ i[np.random.randint(0, 5)] for i in indexed_captions[indices] ]\n",
        "    batch_captions_matrix = batch_captions_to_matrix(batch_captions_matrix, pad_idx, max_len)\n",
        "\n",
        "    return {decoder.img_embeds: batch_image_embeddings, \n",
        "            decoder.sentences: batch_captions_matrix}\n",
        "\n",
        "\n",
        "# prepare vocabulary\n",
        "vocab = generate_vocabulary(train_captions)\n",
        "vocab_inverse = {idx: w for w, idx in vocab.items()}\n",
        "print(\"Length of Vocabulary:\", len(vocab))\n",
        "\n",
        "# replace tokens with indices\n",
        "train_captions_indexed = caption_tokens_to_indices(train_captions, vocab)\n",
        "val_captions_indexed = caption_tokens_to_indices(val_captions, vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sPU-l08JkO0",
        "colab_type": "text"
      },
      "source": [
        "# Define Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbGD3UqrLsU_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define constants\n",
        "IMG_EMBED_SIZE = train_img_embeds.shape[1]\n",
        "IMG_EMBED_BOTTLENECK = 120\n",
        "WORD_EMBED_SIZE = 100\n",
        "LSTM_UNITS = 300\n",
        "LOGIT_BOTTLENECK = 120\n",
        "pad_idx = vocab[PAD]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cftrb-EAJjoD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remember to reset your graph if you want to start building it from scratch!\n",
        "s = reset_tf_session()\n",
        "tf.set_random_seed(42)\n",
        "\n",
        "class decoder:\n",
        "    # [batch_size, IMG_EMBED_SIZE] of CNN image features\n",
        "    img_embeds = tf.placeholder('float32', [None, IMG_EMBED_SIZE])\n",
        "    # [batch_size, time steps] of word ids\n",
        "    sentences = tf.placeholder('int32', [None, None])\n",
        "    \n",
        "    # we use bottleneck here to reduce the number of parameters\n",
        "    # image embedding -> bottleneck\n",
        "    img_embed_to_bottleneck = L.Dense(IMG_EMBED_BOTTLENECK, \n",
        "                                      input_shape=(None, IMG_EMBED_SIZE), \n",
        "                                      activation='elu')\n",
        "    # image embedding bottleneck -> lstm initial state\n",
        "    img_embed_bottleneck_to_h0 = L.Dense(LSTM_UNITS,\n",
        "                                         input_shape=(None, IMG_EMBED_BOTTLENECK),\n",
        "                                         activation='elu')\n",
        "    # word -> embedding\n",
        "    word_embed = L.Embedding(len(vocab), WORD_EMBED_SIZE)\n",
        "    # lstm cell (from tensorflow)\n",
        "    lstm = tf.nn.rnn_cell.LSTMCell(LSTM_UNITS)\n",
        "    \n",
        "    # we use bottleneck here to reduce model complexity\n",
        "    # lstm output -> logits bottleneck\n",
        "    token_logits_bottleneck = L.Dense(LOGIT_BOTTLENECK, \n",
        "                                      input_shape=(None, LSTM_UNITS),\n",
        "                                      activation=\"elu\")\n",
        "    # logits bottleneck -> logits for next token prediction\n",
        "    token_logits = L.Dense(len(vocab),\n",
        "                           input_shape=(None, LOGIT_BOTTLENECK))\n",
        "    \n",
        "    # initial lstm cell state of shape (None, LSTM_UNITS),\n",
        "    # we need to condition it on `img_embeds` placeholder.\n",
        "    c0 = h0 = img_embed_bottleneck_to_h0(img_embed_to_bottleneck(img_embeds))\n",
        "\n",
        "    # embed all tokens but the last for lstm input,\n",
        "    # remember that L.Embedding is callable,\n",
        "    # use `sentences` placeholder as input.\n",
        "    word_embeds = word_embed(sentences[:, :-1])\n",
        "    \n",
        "    # during training we use ground truth tokens `word_embeds` as context for next token prediction.\n",
        "    # that means that we know all the inputs for our lstm and can get \n",
        "    # all the hidden states with one tensorflow operation (tf.nn.dynamic_rnn).\n",
        "    # `hidden_states` has a shape of [batch_size, time steps, LSTM_UNITS].\n",
        "    hidden_states, _ = tf.nn.dynamic_rnn(lstm, word_embeds,\n",
        "                                         initial_state=tf.nn.rnn_cell.LSTMStateTuple(c0, h0))\n",
        "\n",
        "    # now we need to calculate token logits for all the hidden states\n",
        "    \n",
        "    # first, we reshape `hidden_states` to [-1, LSTM_UNITS]\n",
        "    flat_hidden_states = tf.reshape(hidden_states, [-1, LSTM_UNITS])\n",
        "\n",
        "    # then, we calculate logits for next tokens using `token_logits_bottleneck` and `token_logits` layers\n",
        "    flat_token_logits =  token_logits(token_logits_bottleneck(flat_hidden_states))\n",
        "    \n",
        "    # then, we flatten the ground truth token ids.\n",
        "    # remember, that we predict next tokens for each time step,\n",
        "    # use `sentences` placeholder.\n",
        "    flat_ground_truth = tf.reshape(sentences[:, 1:], [-1])\n",
        "\n",
        "    # we need to know where we have real tokens (not padding) in `flat_ground_truth`,\n",
        "    # we don't want to propagate the loss for padded output tokens,\n",
        "    # fill `flat_loss_mask` with 1.0 for real tokens (not pad_idx) and 0.0 otherwise.\n",
        "    flat_loss_mask = tf.not_equal(flat_ground_truth, pad_idx)\n",
        "\n",
        "    # compute cross-entropy between `flat_ground_truth` and `flat_token_logits` predicted by lstm\n",
        "    xent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "        labels=flat_ground_truth, \n",
        "        logits=flat_token_logits\n",
        "    )\n",
        "\n",
        "    # compute average `xent` over tokens with nonzero `flat_loss_mask`.\n",
        "    # we don't want to account misclassification of PAD tokens, because that doesn't make sense,\n",
        "    # we have PAD tokens for batching purposes only!\n",
        "    loss = tf.reduce_mean(tf.boolean_mask(xent, flat_loss_mask))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsQAVqjoLoyb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define optimizer operation to minimize the loss\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
        "train_step = optimizer.minimize(decoder.loss)\n",
        "\n",
        "# will be used to save/load network weights.\n",
        "# you need to reset your default graph and define it in the same way to be able to load the saved weights!\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "# intialize all variables\n",
        "s.run(tf.global_variables_initializer())"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6Qwd4SpdLSe",
        "colab_type": "text"
      },
      "source": [
        "# Load Pre-Trained Decoder Weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0KmrL3ooELP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# you can load trained weights here\n",
        "# you can load \"weights_{epoch}\" and continue training\n",
        "# uncomment the next line if you need to load weights\n",
        "saver.restore(s, os.path.abspath(\"weights\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHSdEwaxL2R2",
        "colab_type": "text"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiFHfn6pL4lO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_captions_indexed = np.array(train_captions_indexed)\n",
        "val_captions_indexed = np.array(val_captions_indexed)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHAUdzx7n__T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 64\n",
        "n_epochs = 12\n",
        "n_batches_per_epoch = 1000\n",
        "n_validation_batches = 100  # how many batches are used for validation after each epoch"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CQA03BKoDL6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# actual training loop\n",
        "MAX_LEN = 20  # truncate long captions to speed up training\n",
        "\n",
        "# to make training reproducible\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    \n",
        "    train_loss = 0\n",
        "    pbar = tqdm_utils.tqdm_notebook_failsafe(range(n_batches_per_epoch))\n",
        "    counter = 0\n",
        "    for _ in pbar:\n",
        "        train_loss += s.run([decoder.loss, train_step], \n",
        "                            generate_batch(train_img_embeds, \n",
        "                                           train_captions_indexed, \n",
        "                                           batch_size, \n",
        "                                           MAX_LEN))[0]\n",
        "        counter += 1\n",
        "        pbar.set_description(\"Training loss: %f\" % (train_loss / counter))\n",
        "        \n",
        "    train_loss /= n_batches_per_epoch\n",
        "    \n",
        "    val_loss = 0\n",
        "    for _ in range(n_validation_batches):\n",
        "        val_loss += s.run(decoder.loss, generate_batch(val_img_embeds,\n",
        "                                                       val_captions_indexed, \n",
        "                                                       batch_size, \n",
        "                                                       MAX_LEN))\n",
        "    val_loss /= n_validation_batches\n",
        "    \n",
        "    print('Epoch: {}, train loss: {}, val loss: {}'.format(epoch, train_loss, val_loss))\n",
        "\n",
        "    # save weights after finishing epoch\n",
        "    saver.save(s, os.path.abspath(\"weights_{}\".format(epoch)))\n",
        "    \n",
        "print(\"Finished!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzX7hgqVIugC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save graph weights to file!\n",
        "saver.save(s, os.path.abspath(\"weights\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3o-mC4BKOFG",
        "colab_type": "text"
      },
      "source": [
        "# Define Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvn6Bc-rhr2P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "IMG_SIZE = 299"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6XOgFP7KOj3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we take the last hidden layer of IncetionV3 as an image embedding\n",
        "def get_cnn_encoder():\n",
        "    K.set_learning_phase(False)\n",
        "    model = keras.applications.InceptionV3(include_top=False)\n",
        "    preprocess_for_model = keras.applications.inception_v3.preprocess_input\n",
        "\n",
        "    model = keras.models.Model(model.inputs, keras.layers.GlobalAveragePooling2D()(model.output))\n",
        "    return model, preprocess_for_model"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3J9a8xhuKP23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class final_model:\n",
        "    # CNN encoder\n",
        "    encoder, preprocess_for_model = get_cnn_encoder()\n",
        "    saver.restore(s, os.path.abspath(\"weights\"))  # keras applications corrupt our graph, so we restore trained weights\n",
        "    \n",
        "    # containers for current lstm state\n",
        "    lstm_c = tf.Variable(tf.zeros([1, LSTM_UNITS]), name=\"cell\")\n",
        "    lstm_h = tf.Variable(tf.zeros([1, LSTM_UNITS]), name=\"hidden\")\n",
        "\n",
        "    # input images\n",
        "    input_images = tf.placeholder('float32', [1, IMG_SIZE, IMG_SIZE, 3], name='images')\n",
        "\n",
        "    # get image embeddings\n",
        "    img_embeds = encoder(input_images)\n",
        "\n",
        "    # initialize lstm state conditioned on image\n",
        "    init_c = init_h = decoder.img_embed_bottleneck_to_h0(decoder.img_embed_to_bottleneck(img_embeds))\n",
        "    init_lstm = tf.assign(lstm_c, init_c), tf.assign(lstm_h, init_h)\n",
        "    \n",
        "    # current word index\n",
        "    current_word = tf.placeholder('int32', [1], name='current_input')\n",
        "\n",
        "    # embedding for current word\n",
        "    word_embed = decoder.word_embed(current_word)\n",
        "\n",
        "    # apply lstm cell, get new lstm states\n",
        "    new_c, new_h = decoder.lstm(word_embed, tf.nn.rnn_cell.LSTMStateTuple(lstm_c, lstm_h))[1]\n",
        "\n",
        "    # compute logits for next token\n",
        "    new_logits = decoder.token_logits(decoder.token_logits_bottleneck(new_h))\n",
        "    # compute probabilities for next token\n",
        "    new_probs = tf.nn.softmax(new_logits)\n",
        "\n",
        "    # `one_step` outputs probabilities of next token and updates lstm hidden state\n",
        "    one_step = new_probs, tf.assign(lstm_c, new_c), tf.assign(lstm_h, new_h)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9yL5vfFKULt",
        "colab_type": "text"
      },
      "source": [
        "# Prediction Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lz2lkUp8KSzn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this is an actual prediction loop\n",
        "def generate_caption(image, t=1, sample=False, max_len=20):\n",
        "    \"\"\"\n",
        "    Generate caption for given image.\n",
        "    if `sample` is True, we will sample next token from predicted probability distribution.\n",
        "    `t` is a temperature during that sampling,\n",
        "        higher `t` causes more uniform-like distribution = more chaos.\n",
        "    \"\"\"\n",
        "    # condition lstm on the image\n",
        "    s.run(final_model.init_lstm, \n",
        "          {final_model.input_images: [image]})\n",
        "    \n",
        "    # current caption\n",
        "    # start with only START token\n",
        "    caption = [vocab[START]]\n",
        "    \n",
        "    for _ in range(max_len):\n",
        "        next_word_probs = s.run(final_model.one_step, \n",
        "                                {final_model.current_word: [caption[-1]]})[0]\n",
        "        next_word_probs = next_word_probs.ravel()\n",
        "        \n",
        "        # apply temperature\n",
        "        next_word_probs = next_word_probs**(1/t) / np.sum(next_word_probs**(1/t))\n",
        "\n",
        "        if sample:\n",
        "            next_word = np.random.choice(range(len(vocab)), p=next_word_probs)\n",
        "        else:\n",
        "            next_word = np.argmax(next_word_probs)\n",
        "\n",
        "        caption.append(next_word)\n",
        "        if next_word == vocab[END]:\n",
        "            break\n",
        "       \n",
        "    return list(map(vocab_inverse.get, caption))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZUSvIQBKnVK",
        "colab_type": "text"
      },
      "source": [
        "# Make Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhAmPZE4KmE7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# look at validation prediction example\n",
        "def apply_model_to_image_raw_bytes(raw):\n",
        "    img = utils.decode_image_from_buf(raw)\n",
        "    fig = plt.figure(figsize=(7, 7))\n",
        "    plt.grid('off')\n",
        "    plt.axis('off')\n",
        "    plt.imshow(img)\n",
        "    img = utils.crop_and_preprocess(img, (IMG_SIZE, IMG_SIZE), final_model.preprocess_for_model)\n",
        "    print(' '.join(generate_caption(img)[1:-1]))\n",
        "    plt.show()\n",
        "\n",
        "def show_valid_example(val_img_fns, example_idx=0):\n",
        "    zf = zipfile.ZipFile(\"data/val2014_sample.zip\")\n",
        "    all_files = set(val_img_fns)\n",
        "    found_files = list(filter(lambda x: x.filename.rsplit(\"/\")[-1] in all_files, zf.filelist))\n",
        "    example = found_files[example_idx]\n",
        "    apply_model_to_image_raw_bytes(zf.read(example))\n",
        "    \n",
        "show_valid_example(val_img_fns, example_idx=115)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}